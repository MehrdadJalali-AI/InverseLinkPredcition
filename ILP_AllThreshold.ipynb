{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehrdadJalali-AI/InverseLinkPredcition/blob/main/ILP_AllThreshold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20aa1b2e-d69a-44e4-871f-e4b2ece5a9e7",
      "metadata": {
        "id": "20aa1b2e-d69a-44e4-871f-e4b2ece5a9e7",
        "outputId": "6d405a5d-3110-42a8-f9e3-23226fbe07e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Mismatch between summary data and node labels.\n",
            "Original graph: 414650 edges, 12561 nodes\n",
            "Best combination coefficient (a): 0.8 with modularity: 0.8328619530154958\n",
            "Features of remaining nodes saved to 'remaining_node_features_0.50.csv'.\n",
            "\n",
            "Threshold: 0.50\n",
            "Threshold: 0.5000\n",
            "Remaining Nodes: 12022\n",
            "Remaining Edges: 245602\n",
            "Node Reduction (%): 4.2911\n",
            "Edge Reduction (%): 40.7688\n",
            "Average Degree: 40.8588\n",
            "Modularity: 0.8336\n",
            "Communities: 770\n",
            "Computation Time (s): 1.1601\n",
            "Features of remaining nodes saved to 'remaining_node_features_0.80.csv'.\n",
            "\n",
            "Threshold: 0.80\n",
            "Threshold: 0.8000\n",
            "Remaining Nodes: 10548\n",
            "Remaining Edges: 89872\n",
            "Node Reduction (%): 16.0258\n",
            "Edge Reduction (%): 78.3258\n",
            "Average Degree: 17.0406\n",
            "Modularity: 0.8357\n",
            "Communities: 694\n",
            "Computation Time (s): 0.9631\n",
            "Features of remaining nodes saved to 'remaining_node_features_0.90.csv'.\n",
            "\n",
            "Threshold: 0.90\n",
            "Threshold: 0.9000\n",
            "Remaining Nodes: 8938\n",
            "Remaining Edges: 38243\n",
            "Node Reduction (%): 28.8432\n",
            "Edge Reduction (%): 90.7770\n",
            "Average Degree: 8.5574\n",
            "Modularity: 0.8466\n",
            "Communities: 687\n",
            "Computation Time (s): 1.0138\n",
            "Features of remaining nodes saved to 'remaining_node_features_0.91.csv'.\n",
            "\n",
            "Threshold: 0.91\n",
            "Threshold: 0.9100\n",
            "Remaining Nodes: 8635\n",
            "Remaining Edges: 33022\n",
            "Node Reduction (%): 31.2555\n",
            "Edge Reduction (%): 92.0362\n",
            "Average Degree: 7.6484\n",
            "Modularity: 0.8483\n",
            "Communities: 672\n",
            "Computation Time (s): 1.2803\n",
            "Features of remaining nodes saved to 'remaining_node_features_0.92.csv'.\n",
            "\n",
            "Threshold: 0.92\n",
            "Threshold: 0.9200\n",
            "Remaining Nodes: 8318\n",
            "Remaining Edges: 27809\n",
            "Node Reduction (%): 33.7792\n",
            "Edge Reduction (%): 93.2934\n",
            "Average Degree: 6.6865\n",
            "Modularity: 0.8504\n",
            "Communities: 691\n",
            "Computation Time (s): 1.0278\n",
            "Features of remaining nodes saved to 'remaining_node_features_0.93.csv'.\n",
            "\n",
            "Threshold: 0.93\n",
            "Threshold: 0.9300\n",
            "Remaining Nodes: 7868\n",
            "Remaining Edges: 22649\n",
            "Node Reduction (%): 37.3617\n",
            "Edge Reduction (%): 94.5378\n",
            "Average Degree: 5.7572\n",
            "Modularity: 0.8546\n",
            "Communities: 705\n",
            "Computation Time (s): 1.2709\n",
            "Features of remaining nodes saved to 'remaining_node_features_0.94.csv'.\n",
            "\n",
            "Threshold: 0.94\n",
            "Threshold: 0.9400\n",
            "Remaining Nodes: 7322\n",
            "Remaining Edges: 17666\n",
            "Node Reduction (%): 41.7085\n",
            "Edge Reduction (%): 95.7395\n",
            "Average Degree: 4.8255\n",
            "Modularity: 0.8623\n",
            "Communities: 720\n",
            "Computation Time (s): 1.0392\n",
            "Features of remaining nodes saved to 'remaining_node_features_0.95.csv'.\n",
            "\n",
            "Threshold: 0.95\n",
            "Threshold: 0.9500\n",
            "Remaining Nodes: 6510\n",
            "Remaining Edges: 12503\n",
            "Node Reduction (%): 48.1729\n",
            "Edge Reduction (%): 96.9847\n",
            "Average Degree: 3.8412\n",
            "Modularity: 0.8732\n",
            "Communities: 715\n",
            "Computation Time (s): 1.0214\n",
            "Features of remaining nodes saved to 'remaining_node_features_0.96.csv'.\n",
            "\n",
            "Threshold: 0.96\n",
            "Threshold: 0.9600\n",
            "Remaining Nodes: 5150\n",
            "Remaining Edges: 7391\n",
            "Node Reduction (%): 59.0001\n",
            "Edge Reduction (%): 98.2175\n",
            "Average Degree: 2.8703\n",
            "Modularity: 0.8984\n",
            "Communities: 750\n",
            "Computation Time (s): 1.0367\n",
            "Features of remaining nodes saved to 'remaining_node_features_0.97.csv'.\n",
            "\n",
            "Threshold: 0.97\n",
            "Threshold: 0.9700\n",
            "Remaining Nodes: 3441\n",
            "Remaining Edges: 3745\n",
            "Node Reduction (%): 72.6057\n",
            "Edge Reduction (%): 99.0968\n",
            "Average Degree: 2.1767\n",
            "Modularity: 0.9274\n",
            "Communities: 720\n",
            "Computation Time (s): 1.0321\n",
            "Features of remaining nodes saved to 'remaining_node_features_0.98.csv'.\n",
            "\n",
            "Threshold: 0.98\n",
            "Threshold: 0.9800\n",
            "Remaining Nodes: 2133\n",
            "Remaining Edges: 1900\n",
            "Node Reduction (%): 83.0189\n",
            "Edge Reduction (%): 99.5418\n",
            "Average Degree: 1.7815\n",
            "Modularity: 0.9375\n",
            "Communities: 563\n",
            "Computation Time (s): 1.0302\n",
            "Features of remaining nodes saved to 'remaining_node_features_0.99.csv'.\n",
            "\n",
            "Threshold: 0.99\n",
            "Threshold: 0.9900\n",
            "Remaining Nodes: 1138\n",
            "Remaining Edges: 829\n",
            "Node Reduction (%): 90.9402\n",
            "Edge Reduction (%): 99.8001\n",
            "Average Degree: 1.4569\n",
            "Modularity: 0.9350\n",
            "Communities: 377\n",
            "Computation Time (s): 0.9978\n",
            "\n",
            "All results saved to 'sparsification_results.csv'\n",
            "Total runtime: 344.40 seconds\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras import layers, models\n",
        "import networkx.algorithms.community as nx_comm\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "# Suppress RDKit warnings\n",
        "from rdkit import rdBase\n",
        "rdBase.DisableLog(\"rdApp.*\")\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def load_edges_list(filename):\n",
        "    \"\"\"Loads edge list from a CSV file. O(m)\"\"\"\n",
        "    try:\n",
        "        return pd.read_csv(filename)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Edge list file '{filename}' not found.\")\n",
        "\n",
        "def generate_fingerprint(smiles):\n",
        "    \"\"\"Generates a molecular fingerprint from a SMILES string. O(1) per call\"\"\"\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return np.zeros(1024, dtype=np.float32)\n",
        "        return np.array(AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024), dtype=np.float32)\n",
        "    except Exception as e:\n",
        "        print(f\"SMILES Parse Error: {e}\")\n",
        "        return np.zeros(1024, dtype=np.float32)\n",
        "\n",
        "def label_encode_metal_names(metal_names):\n",
        "    \"\"\"Encodes metal names as integers. O(n)\"\"\"\n",
        "    unique_metals = np.unique(metal_names)\n",
        "    metal_dict = {metal: idx for idx, metal in enumerate(unique_metals)}\n",
        "    return np.array([metal_dict[metal] for metal in metal_names], dtype=np.int32)\n",
        "\n",
        "def load_summary_data(filename, node_labels):\n",
        "    \"\"\"Loads and preprocesses summary data to extract features. O(n * f)\"\"\"\n",
        "    try:\n",
        "        summary_data = pd.read_csv(filename, index_col=0)\n",
        "        linker_smiles = summary_data['linker SMILES']\n",
        "        linker_features = np.stack(linker_smiles.apply(generate_fingerprint).values)\n",
        "        metal_features = label_encode_metal_names(summary_data['metal']).reshape(-1, 1)\n",
        "        other_features = summary_data[['Largest Cavity Diameter', 'Pore Limiting Diameter',\n",
        "                                    'Largest Free Sphere']].astype(np.float32).values\n",
        "        features = np.concatenate((linker_features, metal_features, other_features), axis=1)\n",
        "\n",
        "        if (len(summary_data) == len(node_labels) and\n",
        "            all(label in summary_data.index for label in node_labels)):\n",
        "            return pd.DataFrame(features, index=node_labels)\n",
        "        else:\n",
        "            print(\"Warning: Mismatch between summary data and node labels.\")\n",
        "            return pd.DataFrame(features, index=summary_data.index)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Summary data file '{filename}' not found.\")\n",
        "\n",
        "def build_and_compile_model(input_shape):\n",
        "    \"\"\"Builds and compiles a neural network model. O(1) (no training here)\"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(input_shape,)),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return model\n",
        "\n",
        "def predict_link_importance(model, node_features):\n",
        "    \"\"\"Predicts link importance using a trained model. O(n * f * h)\"\"\"\n",
        "    return model.predict(node_features, verbose=0).flatten()\n",
        "\n",
        "def optimize_combination_for_modularity(graph, features, importance_scores, initial_weights, threshold=0.2):\n",
        "    \"\"\"Optimizes combination for modularity. O(m * log(n))\"\"\"\n",
        "    best_modularity, best_a = -1, 0.0\n",
        "    a_range = np.linspace(0, 1, 11)\n",
        "\n",
        "    for a in a_range:  # 11 iterations, O(1)\n",
        "        combined_scores = a * importance_scores + (1 - a) * initial_weights  # O(m)\n",
        "        edges_to_remove = [edge for edge, score in zip(graph.edges(), combined_scores) if score < threshold]  # O(m)\n",
        "        graph_copy = graph.copy()  # O(m + n)\n",
        "        graph_copy.remove_edges_from(edges_to_remove)  # O(m)\n",
        "        communities = nx_comm.greedy_modularity_communities(graph_copy)  # O(m * log(n))\n",
        "        modularity = nx_comm.modularity(graph_copy, communities)  # O(m + n)\n",
        "        if modularity > best_modularity:\n",
        "            best_modularity, best_a = modularity, a\n",
        "\n",
        "    print(f\"Best combination coefficient (a): {best_a} with modularity: {best_modularity}\")\n",
        "    return best_a\n",
        "\n",
        "def remove_edges_and_nodes(graph, importance_scores, initial_weights, best_a, threshold):\n",
        "    \"\"\"Removes edges and nodes based on combined scores. O(m + n)\"\"\"\n",
        "    combined_scores = best_a * importance_scores + (1 - best_a) * initial_weights  # O(m)\n",
        "    edges_to_remove = [(u, v) for (u, v, _), score in zip(graph.edges(data=True), combined_scores) if score < threshold]  # O(m)\n",
        "    graph.remove_edges_from(edges_to_remove)  # O(m)\n",
        "    graph.remove_nodes_from(list(nx.isolates(graph)))  # O(n)\n",
        "    return graph\n",
        "def save_remaining_node_features(graph, summary_data, filename):\n",
        "    \"\"\"Saves features of remaining nodes to a file.\"\"\"\n",
        "    remaining_nodes = list(graph.nodes())\n",
        "    remaining_features = summary_data.loc[remaining_nodes]\n",
        "    remaining_features.to_csv(filename, index=False)\n",
        "    print(f\"Features of remaining nodes saved to '{filename}'.\")\n",
        "\n",
        "def calculate_metrics(original_graph, sparsified_graph, computation_time):\n",
        "    \"\"\"Calculates metrics for the sparsified graph. O(m * log(n))\"\"\"\n",
        "    orig_nodes, orig_edges = original_graph.number_of_nodes(), original_graph.number_of_edges()  # O(1)\n",
        "    new_nodes, new_edges = sparsified_graph.number_of_nodes(), sparsified_graph.number_of_edges()  # O(1)\n",
        "\n",
        "    node_reduction = ((orig_nodes - new_nodes) / orig_nodes) * 100 if orig_nodes > 0 else 0  # O(1)\n",
        "    edge_reduction = ((orig_edges - new_edges) / orig_edges) * 100 if orig_edges > 0 else 0  # O(1)\n",
        "    avg_degree = sum(d for _, d in sparsified_graph.degree()) / new_nodes if new_nodes > 0 else 0  # O(n)\n",
        "    communities = nx_comm.greedy_modularity_communities(sparsified_graph)  # O(m * log(n))\n",
        "    modularity = nx_comm.modularity(sparsified_graph, communities) if new_edges > 0 else 0  # O(m + n)\n",
        "\n",
        "    return {\n",
        "        'Threshold': None,\n",
        "        'Remaining Nodes': new_nodes,\n",
        "        'Remaining Edges': new_edges,\n",
        "        'Node Reduction (%)': node_reduction,\n",
        "        'Edge Reduction (%)': edge_reduction,\n",
        "        'Average Degree': avg_degree,\n",
        "        'Modularity': modularity,\n",
        "        'Communities': len(communities),\n",
        "        'Computation Time (s)': computation_time\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function. O(T * m * log(n)) overall\"\"\"\n",
        "    start_time = time.time()\n",
        "    config = {\n",
        "        'edges_list_filename': 'edges_list_0.8_Full_2.csv',\n",
        "        'summary_data_filename': '1M1L3D_summary.csv',\n",
        "        'output_base': 'sparsified_graph_edges',\n",
        "        'features_base': 'remaining_node_features',\n",
        "        'thresholds': [0.5, 0.8, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
        "    }\n",
        "\n",
        "    # Load and prepare data: O(m + n * f)\n",
        "    edges_list = load_edges_list(config['edges_list_filename'])\n",
        "    node_labels = pd.concat([edges_list['source'], edges_list['target']]).unique()\n",
        "    summary_data = load_summary_data(config['summary_data_filename'], node_labels)\n",
        "\n",
        "    # Build original graph: O(m)\n",
        "    original_graph = nx.Graph()\n",
        "    for _, row in edges_list.iterrows():\n",
        "        original_graph.add_edge(row['source'], row['target'], weight=row['weight'])\n",
        "    print(f\"Original graph: {original_graph.number_of_edges()} edges, {original_graph.number_of_nodes()} nodes\")\n",
        "\n",
        "    # Prepare features: O(n * f)\n",
        "    node_features = np.array([summary_data.loc[node].values for node in original_graph.nodes()])\n",
        "    scaler = StandardScaler()\n",
        "    node_features_scaled = scaler.fit_transform(node_features)\n",
        "\n",
        "    # Build model (training placeholder): O(1) currently\n",
        "    model = build_and_compile_model(node_features_scaled.shape[1])\n",
        "    # Hypothetical training: O(e * n * f * h) if implemented\n",
        "\n",
        "    # Generate dummy scores (edge weight calc): O(m) currently, O(n * f * h + m) with model\n",
        "    importance_scores = np.random.rand(len(original_graph.edges()))\n",
        "    initial_weights = np.array([data['weight'] for _, _, data in original_graph.edges(data=True)])\n",
        "\n",
        "    # Optimize combination: O(m * log(n))\n",
        "    best_a = optimize_combination_for_modularity(original_graph, summary_data, importance_scores, initial_weights)\n",
        "\n",
        "    # Results storage\n",
        "    results = []\n",
        "\n",
        "    # Iterate over thresholds: O(T * (m + n + m * log(n)))\n",
        "    for threshold in config['thresholds']:\n",
        "        threshold_start_time = time.time()\n",
        "\n",
        "        graph = original_graph.copy()  # O(m + n)\n",
        "        graph = remove_edges_and_nodes(graph, importance_scores, initial_weights, best_a, threshold)  # O(m + n)\n",
        "        computation_time = time.time() - threshold_start_time\n",
        "\n",
        "        metrics = calculate_metrics(original_graph, graph, computation_time)  # O(m * log(n))\n",
        "        metrics['Threshold'] = threshold\n",
        "        results.append(metrics)\n",
        "\n",
        "        output_edges_filename = f\"{config['output_base']}_{threshold:.2f}.csv\"\n",
        "        output_features_filename = f\"{config['features_base']}_{threshold:.2f}.csv\"\n",
        "        nx.write_edgelist(graph, output_edges_filename, data=['weight'])  # O(m)\n",
        "        save_remaining_node_features(graph, summary_data, output_features_filename)  # O(n)\n",
        "\n",
        "        print(f\"\\nThreshold: {threshold:.2f}\")\n",
        "        for key, value in metrics.items():\n",
        "            if isinstance(value, float):\n",
        "                print(f\"{key}: {value:.4f}\")\n",
        "            else:\n",
        "                print(f\"{key}: {value}\")\n",
        "\n",
        "    # Save results: O(T)\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df = results_df[['Threshold', 'Remaining Nodes', 'Remaining Edges',\n",
        "                            'Node Reduction (%)', 'Edge Reduction (%)',\n",
        "                            'Average Degree', 'Modularity', 'Communities',\n",
        "                            'Computation Time (s)']]\n",
        "    results_df.to_csv('sparsification_results.csv', index=False)\n",
        "    print(f\"\\nAll results saved to 'sparsification_results.csv'\")\n",
        "    print(f\"Total runtime: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a37ce88-4469-4de0-aca2-b4f636f04727",
      "metadata": {
        "id": "4a37ce88-4469-4de0-aca2-b4f636f04727"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}